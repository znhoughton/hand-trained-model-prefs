---
title: "analysis-script"
output:
  html_document: default
  pdf_document: default
date: "2026-01-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Model analyses

### load human data

```{r}

library(tidyverse)

babylm_trigrams =
  read_csv("../Data/babylm_eng_trigrams.csv") %>%
  mutate(trigram = tolower(trigram))

all_human_data = read_csv('../Data/all_human_data.csv') %>%
  mutate(Attested = ifelse(
    OverallFreq == 0, 0, 1
  )) %>%
  select(participant, resp, Word1, Word2, Alpha, Nonalpha, OverallFreq, Word1_freq, Word2_freq, Form, Percept, Culture, Power, Intense, Icon, Freq, Len, Lapse, "*BStress", RelFreq, GenPref, Attested) %>%
  mutate(y_vals = 0.02191943 + 0.23925834*Form +  0.24889543*Percept +  0.41836997*Culture +   0.25967334*Power +  0.01867604*Intense +  1.30365980*Icon +   0.08553552*Freq +  0.15241566*Len - 0.19381657*Lapse +  0.36019221*`*BStress`) %>%
  mutate(GenPref = 1/(1+exp(-1*y_vals))) %>%
  mutate(GenPref = GenPref - 0.5)

all_human_data = all_human_data %>%
  mutate(resp_binary = case_when(
    resp == 'alpha' ~ 1,
    resp == 'nonalpha' ~ 0
  ))

all_human_data_novel    <- all_human_data %>% filter(Attested == 0)
all_human_data_attested <- all_human_data %>% filter(Attested == 1)



clean_word = function(x) {
  x %>%
    tolower() %>%
    stringr::str_remove_all("^[[:punct:]]+|[[:punct:]]+$")
}

all_human_data =
  all_human_data %>%
  mutate(
    Word1_clean = clean_word(Word1),
    Word2_clean = clean_word(Word2)
  )

all_human_data =
  all_human_data %>%
  mutate(
    babyLM_alpha_trigram    = paste(Word1_clean, "and", Word2_clean),
    babyLM_nonalpha_trigram = paste(Word2_clean, "and", Word1_clean)
  )

all_human_data =
  all_human_data %>%
  left_join(
    babylm_trigrams,
    by = c("babyLM_alpha_trigram" = "trigram")
  ) %>%
  rename(babyLMalphacount = count)

all_human_data =
  all_human_data %>%
  left_join(
    babylm_trigrams,
    by = c("babyLM_nonalpha_trigram" = "trigram")
  ) %>%
  rename(babyLMnonalphacount = count)


all_human_data =
  all_human_data %>%
  mutate(
    babyLMalphacount    = coalesce(babyLMalphacount, 0L),
    babyLMnonalphacount = coalesce(babyLMnonalphacount, 0L)
  )


babylm_unigrams =
  read_csv("../Data/babylm_eng_unigrams.csv") %>%
  mutate(word = tolower(word))

all_human_data = all_human_data %>%
  left_join(
    babylm_unigrams,
    by = c("Word1_clean" = "word")
  ) %>%
  rename(Word1BabyLMCount = count) %>%
  left_join(
    babylm_unigrams,
    by = c("Word2_clean" = "word")
  ) %>%
  rename(Word2BabyLMCount = count) %>%
  mutate(
    Word1BabyLMCount = coalesce(Word1BabyLMCount, 0L),
    Word2BabyLMCount = coalesce(Word2BabyLMCount, 0L)
  )

all_human_data = all_human_data %>%
  filter(Word1BabyLMCount > 0 & Word2BabyLMCount > 0)
```


### load llm data

```{r}

library(brms)
out_dir = "../Data/checkpoint_results"

llm_data = list.files(out_dir, pattern = "\\.csv$", full.names = TRUE) %>%
  map_dfr(read_csv, na = character(), trim_ws = FALSE, show_col_types = FALSE)

colnames(llm_data)
colnames(all_human_data)

llm_data = llm_data %>%
  group_by(model, binom, checkpoint, step, tokens) %>%
  summarize(preference = mean(preference))

all_human_data = all_human_data %>%
  mutate(binom = Alpha)

avg_human_pref = all_human_data %>%
  group_by(binom, Word1, Word2, Alpha, Nonalpha, OverallFreq, Word1_freq, Word2_freq, Form, Percept, Culture, Power, Intense, Icon, Freq, Len, Lapse, `*BStress`, RelFreq, GenPref, Attested, y_vals, Word1_clean, Word2_clean, babyLM_alpha_trigram, babyLM_nonalpha_trigram, babyLMalphacount, babyLMnonalphacount, Word1BabyLMCount, Word2BabyLMCount) %>%
  summarize(
    hum_pref = mean(resp_binary),
    .groups = "drop"
  )

llm_and_human_data = llm_data %>%
  left_join(avg_human_pref)

llm_and_human_data_nonce = llm_and_human_data %>%
  filter(Attested == 0) 

llm_and_human_data_attested = llm_and_human_data %>%
  filter(Attested == 1)
```


### Plot

```{r}
llm_and_human_data_nonce_plot = llm_and_human_data_nonce %>%
  group_by(model, checkpoint, tokens) %>%
  summarize(hum_model_cor = cor(preference, hum_pref))

llm_and_human_data_attested_plot = llm_and_human_data_attested %>%
  group_by(model, checkpoint, tokens) %>%
  summarize(hum_model_cor = cor(preference, hum_pref))

ggplot(llm_and_human_data_nonce_plot %>% arrange(tokens),
       aes(x = tokens, y = hum_model_cor)) +
  geom_point(size = 2) +
  geom_smooth(se = FALSE) +
  facet_wrap(~ model) +
  labs(
    x = "Training tokens",
    y = "Correlation (human vs model)",
    title = "Nonce Binoms"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )


ggplot(llm_and_human_data_attested_plot %>% arrange(tokens),
       aes(x = tokens, y = hum_model_cor)) +
  geom_point(size = 2) +
  geom_smooth(se = FALSE) +
  facet_wrap(~ model) +
  labs(
    x = "Training tokens",
    y = "Correlation (human vs model)",
    title = "Attested Binoms"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

```


### Frequency Analysis for Attested Binomials

```{r}
# Add frequency tertiles for attested binomials
llm_and_human_data_attested = llm_and_human_data_attested %>%
  ungroup() %>%
  mutate(freq_tertile = ntile(log(OverallFreq), 5)) %>%
  mutate(freq_category = case_when(
    freq_tertile == 1 ~ "Low Frequency",
    freq_tertile == 2 ~ "Low-Mid Frequency",
    freq_tertile == 3 ~ "Mid Frequency",
    freq_tertile == 4 ~ "High-Mid Frequency",
    freq_tertile == 5 ~ "High Frequency"
  ))

# Calculate correlations by frequency category
freq_analysis = llm_and_human_data_attested %>%
  group_by(model, checkpoint, tokens, freq_category) %>%
  summarize(
    hum_model_cor = cor(preference, hum_pref),
    n_items = n(),
    .groups = "drop"
  )

# Plot correlation by frequency category
ggplot(freq_analysis %>% arrange(tokens),
       aes(x = tokens, y = hum_model_cor, color = freq_category)) +
  geom_point(size = 1.5, alpha = 0.6) +
  geom_smooth(se = FALSE, linewidth = 1.2) +
  facet_wrap(~ model) +
  scale_color_manual(
    values = c("Low Frequency" = "#D55E00",
               "Mid Frequency" = "#009E73",
               "High Frequency" = "#0072B2")
  ) +
  labs(
    x = "Training tokens",
    y = "Correlation (human vs model)",
    title = "Attested Binomials: Correlation by Frequency",
    color = "Binomial\nFrequency"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank(),
    legend.position = "bottom"
  )




# Print summary statistics
freq_summary = freq_analysis %>%
  group_by(model, freq_category) %>%
  summarize(
    final_cor = last(hum_model_cor),
    mean_cor = mean(hum_model_cor),
    .groups = "drop"
  )



print("Final correlations by frequency category:")
print(freq_summary)
```

