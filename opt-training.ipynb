{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ae5ac7-df45-437f-b8f3-af63f57bf18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiwis → ['ki', 'w', 'is']\n",
      "wolverines → ['w', 'ol', 'ver', 'ines']\n",
      "wug → ['w', 'ug']\n",
      "wugs → ['w', 'ugs']\n",
      "blurgidy → ['bl', 'ur', 'g', 'id', 'y']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"models/opt-babylm-100m-bpe\"   # or unigram tokenizer path\n",
    ")\n",
    "\n",
    "words = [\"kiwis\", \"wolverines\", \"wug\", \"wugs\", \"blurgidy\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"→\", tokenizer.tokenize(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59d6d97-f8e3-4501-9123-2aa86915c9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TOKENIZATION TEST\n",
      "==================================================\n",
      "Input text: 'The boy went outside to fly his'\n",
      "Token IDs: [1, 357, 652, 716, 1662, 192, 2024, 305]\n",
      "Tokens: ['<s>', 'the', 'Ġboy', 'Ġwent', 'Ġoutside', 'Ġto', 'Ġfly', 'Ġhis']\n",
      "Decoded: '<s>the boy went outside to fly his'\n",
      "\n",
      "==================================================\n",
      "SPECIAL TOKENS CHECK\n",
      "==================================================\n",
      "BOS: '<s>' (ID: 1)\n",
      "EOS: '</s>' (ID: 2)\n",
      "PAD: '<pad>' (ID: 0)\n",
      "UNK: '<unk>' (ID: 3)\n",
      "Vocab size: 8192\n",
      "\n",
      "==================================================\n",
      "NEXT WORD PREDICTION\n",
      "==================================================\n",
      "Top 10 next word predictions:\n",
      "  1. ' head' (ID: 752, prob: 0.0219)\n",
      "  2. ' hand' (ID: 625, prob: 0.0183)\n",
      "  3. ' own' (ID: 881, prob: 0.0174)\n",
      "  4. ' eyes' (ID: 1099, prob: 0.0165)\n",
      "  5. ' father' (ID: 938, prob: 0.0124)\n",
      "  6. ' hands' (ID: 1172, prob: 0.0116)\n",
      "  7. ' mother' (ID: 826, prob: 0.0115)\n",
      "  8. ' face' (ID: 1106, prob: 0.0097)\n",
      "  9. ' feet' (ID: 1476, prob: 0.0076)\n",
      "  10. ' way' (ID: 582, prob: 0.0072)\n",
      "\n",
      "==================================================\n",
      "LOGITS FOR EACH INPUT TOKEN\n",
      "==================================================\n",
      "Logits shape: torch.Size([1, 8, 8192])\n",
      "(batch_size=1, sequence_length=8, vocab_size=8192)\n",
      "\n",
      "Position 0: '<s>' (ID: 1)\n",
      "  Top prediction: '*' (ID: 13, logit: 10.2652)\n",
      "  Logit range: [-3.63, 10.27]\n",
      "  Mean logit: -1.2461\n",
      "\n",
      "Position 1: 'the' (ID: 357)\n",
      "  Top prediction: ' other' (ID: 537, logit: 5.4732)\n",
      "  Logit range: [-4.03, 5.47]\n",
      "  Mean logit: -0.7592\n",
      "\n",
      "Position 2: ' boy' (ID: 652)\n",
      "  Top prediction: '.' (ID: 17, logit: 7.3192)\n",
      "  Logit range: [-4.55, 7.32]\n",
      "  Mean logit: -1.5391\n",
      "\n",
      "Position 3: ' went' (ID: 716)\n",
      "  Top prediction: ' to' (ID: 192, logit: 7.8542)\n",
      "  Logit range: [-4.84, 7.85]\n",
      "  Mean logit: -1.4328\n",
      "\n",
      "Position 4: ' outside' (ID: 1662)\n",
      "  Top prediction: '.' (ID: 17, logit: 7.8976)\n",
      "  Logit range: [-5.48, 7.90]\n",
      "  Mean logit: -1.7497\n",
      "\n",
      "Position 5: ' to' (ID: 192)\n",
      "  Top prediction: ' the' (ID: 170, logit: 7.3674)\n",
      "  Logit range: [-4.60, 7.37]\n",
      "  Mean logit: -1.0772\n",
      "\n",
      "Position 6: ' fly' (ID: 2024)\n",
      "  Top prediction: ' and' (ID: 203, logit: 7.9736)\n",
      "  Logit range: [-5.60, 7.97]\n",
      "  Mean logit: -1.7467\n",
      "\n",
      "Position 7: ' his' (ID: 305)\n",
      "  Top prediction: ' head' (ID: 752, logit: 6.0914)\n",
      "  Logit range: [-3.71, 6.09]\n",
      "  Mean logit: -0.4200\n",
      "\n",
      "==================================================\n",
      "GENERATION TEST\n",
      "==================================================\n",
      "Input: 'The boy went outside to fly his'\n",
      "Generated: 'the boy went outside to fly his head.that would be going to go.that was going to make the other people to'\n",
      "\n",
      "==================================================\n",
      "PERPLEXITY CHECK (on input)\n",
      "==================================================\n",
      "Loss: 6.0093\n",
      "Perplexity: 407.2066\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_name = \"znhoughton/opt-babylm-125m-seed42\"  # or your local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Test text\n",
    "text = \"The boy went outside to fly his\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TOKENIZATION TEST\")\n",
    "print(\"=\"*50)\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"Token IDs: {encoded['input_ids'].tolist()[0]}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(encoded['input_ids'][0])}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPECIAL TOKENS CHECK\")\n",
    "print(\"=\"*50)\n",
    "print(f\"BOS: '{tokenizer.bos_token}' (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NEXT WORD PREDICTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Get predictions for the last token\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Get top 10 predictions\n",
    "    top_k = 10\n",
    "    probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    print(f\"Top {top_k} next word predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  {i+1}. '{token}' (ID: {idx.item()}, prob: {prob.item():.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOGITS FOR EACH INPUT TOKEN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "    logits = outputs.logits  # Shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"(batch_size={logits.shape[0]}, sequence_length={logits.shape[1]}, vocab_size={logits.shape[2]})\")\n",
    "    \n",
    "    for pos in range(logits.shape[1]):\n",
    "        token_id = encoded['input_ids'][0, pos].item()\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        \n",
    "        # Get logits for this position\n",
    "        position_logits = logits[0, pos, :]\n",
    "        \n",
    "        # Get top prediction at this position\n",
    "        predicted_id = torch.argmax(position_logits).item()\n",
    "        predicted_token = tokenizer.decode([predicted_id])\n",
    "        predicted_logit = position_logits[predicted_id].item()\n",
    "        \n",
    "        print(f\"\\nPosition {pos}: '{token_str}' (ID: {token_id})\")\n",
    "        print(f\"  Top prediction: '{predicted_token}' (ID: {predicted_id}, logit: {predicted_logit:.4f})\")\n",
    "        print(f\"  Logit range: [{position_logits.min().item():.2f}, {position_logits.max().item():.2f}]\")\n",
    "        print(f\"  Mean logit: {position_logits.mean().item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATION TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate continuation\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "generated = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Generated: '{generated_text}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERPLEXITY CHECK (on input)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Calculate loss/perplexity\n",
    "    outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "    \n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Perplexity: {perplexity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8053ae3-b326-49e5-b399-9b130e049bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421689ad66ed49489b2dacde1ce5c2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARING PHRASE PROBABILITIES\n",
      "======================================================================\n",
      "\n",
      "Phrase: 'he likes bread and butter'\n",
      "  Token: 'he' | Prob: 0.005582 | Log-prob: -5.1883\n",
      "  Token: ' likes' | Prob: 0.000898 | Log-prob: -7.0155\n",
      "  Token: ' bread' | Prob: 0.000114 | Log-prob: -9.0836\n",
      "  Token: ' and' | Prob: 0.127637 | Log-prob: -2.0586\n",
      "  Token: ' butter' | Prob: 0.146677 | Log-prob: -1.9195\n",
      "  → Total log-probability: -25.2655\n",
      "  → Probability: 1.06e-11\n",
      "  → Perplexity: 156.5062\n",
      "  → Avg token probability: 0.006390\n",
      "\n",
      "Phrase: 'he likes butter and bread'\n",
      "  Token: 'he' | Prob: 0.005582 | Log-prob: -5.1883\n",
      "  Token: ' likes' | Prob: 0.000898 | Log-prob: -7.0155\n",
      "  Token: ' butter' | Prob: 0.000091 | Log-prob: -9.3060\n",
      "  Token: ' and' | Prob: 0.026030 | Log-prob: -3.6485\n",
      "  Token: ' bread' | Prob: 0.020235 | Log-prob: -3.9004\n",
      "  → Total log-probability: -29.0586\n",
      "  → Probability: 2.40e-13\n",
      "  → Perplexity: 334.1950\n",
      "  → Avg token probability: 0.002992\n",
      "\n",
      "Phrase: 'the cat slept on the mat'\n",
      "  Token: 'the' | Prob: 0.011653 | Log-prob: -4.4522\n",
      "  Token: ' cat' | Prob: 0.000738 | Log-prob: -7.2112\n",
      "  Token: ' slept' | Prob: 0.000578 | Log-prob: -7.4557\n",
      "  Token: ' on' | Prob: 0.085135 | Log-prob: -2.4635\n",
      "  Token: ' the' | Prob: 0.347002 | Log-prob: -1.0584\n",
      "  Token: ' mat' | Prob: 0.003154 | Log-prob: -5.7592\n",
      "  → Total log-probability: -28.4001\n",
      "  → Probability: 4.63e-13\n",
      "  → Perplexity: 113.6767\n",
      "  → Avg token probability: 0.008797\n",
      "\n",
      "Phrase: 'it's not all black and white'\n",
      "  Token: 'it' | Prob: 0.010112 | Log-prob: -4.5940\n",
      "  Token: ''s' | Prob: 0.605481 | Log-prob: -0.5017\n",
      "  Token: ' not' | Prob: 0.111664 | Log-prob: -2.1923\n",
      "  Token: ' all' | Prob: 0.010387 | Log-prob: -4.5672\n",
      "  Token: ' black' | Prob: 0.000316 | Log-prob: -8.0589\n",
      "  Token: ' and' | Prob: 0.038828 | Log-prob: -3.2486\n",
      "  Token: ' white' | Prob: 0.417819 | Log-prob: -0.8727\n",
      "  → Total log-probability: -24.0354\n",
      "  → Probability: 3.64e-11\n",
      "  → Perplexity: 30.9890\n",
      "  → Avg token probability: 0.032270\n",
      "\n",
      "Phrase: 'it's not all white and black'\n",
      "  Token: 'it' | Prob: 0.010112 | Log-prob: -4.5940\n",
      "  Token: ''s' | Prob: 0.605481 | Log-prob: -0.5017\n",
      "  Token: ' not' | Prob: 0.111664 | Log-prob: -2.1923\n",
      "  Token: ' all' | Prob: 0.010387 | Log-prob: -4.5672\n",
      "  Token: ' white' | Prob: 0.000480 | Log-prob: -7.6418\n",
      "  Token: ' and' | Prob: 0.013359 | Log-prob: -4.3156\n",
      "  Token: ' black' | Prob: 0.047254 | Log-prob: -3.0522\n",
      "  → Total log-probability: -26.8648\n",
      "  → Probability: 2.15e-12\n",
      "  → Perplexity: 46.4245\n",
      "  → Avg token probability: 0.021540\n",
      "\n",
      "Phrase: 'I turned on the radios and televisions'\n",
      "  Token: ' turned' | Prob: 0.000000 | Log-prob: -15.4410\n",
      "  Token: ' on' | Prob: 0.028087 | Log-prob: -3.5724\n",
      "  Token: ' the' | Prob: 0.260274 | Log-prob: -1.3460\n",
      "  Token: ' rad' | Prob: 0.000824 | Log-prob: -7.1011\n",
      "  Token: 'ios' | Prob: 0.117878 | Log-prob: -2.1381\n",
      "  Token: ' and' | Prob: 0.088573 | Log-prob: -2.4239\n",
      "  Token: ' tele' | Prob: 0.000353 | Log-prob: -7.9499\n",
      "  Token: 'v' | Prob: 0.010371 | Log-prob: -4.5687\n",
      "  Token: 'isions' | Prob: 0.303181 | Log-prob: -1.1934\n",
      "  → Total log-probability: -45.7345\n",
      "  → Probability: 1.37e-20\n",
      "  → Perplexity: 161.0341\n",
      "  → Avg token probability: 0.006210\n",
      "\n",
      "Phrase: 'I turned on the televisions and radios'\n",
      "  Token: ' turned' | Prob: 0.000000 | Log-prob: -15.4410\n",
      "  Token: ' on' | Prob: 0.028087 | Log-prob: -3.5724\n",
      "  Token: ' the' | Prob: 0.260274 | Log-prob: -1.3460\n",
      "  Token: ' tele' | Prob: 0.000240 | Log-prob: -8.3359\n",
      "  Token: 'v' | Prob: 0.007955 | Log-prob: -4.8340\n",
      "  Token: 'isions' | Prob: 0.511173 | Log-prob: -0.6710\n",
      "  Token: ' and' | Prob: 0.058344 | Log-prob: -2.8414\n",
      "  Token: ' rad' | Prob: 0.000029 | Log-prob: -10.4319\n",
      "  Token: 'ios' | Prob: 0.010721 | Log-prob: -4.5356\n",
      "  → Total log-probability: -52.0092\n",
      "  → Probability: 2.59e-23\n",
      "  → Perplexity: 323.3701\n",
      "  → Avg token probability: 0.003092\n",
      "\n",
      "======================================================================\n",
      "RANKING (Best to Worst)\n",
      "======================================================================\n",
      "1. 'it's not all black and white'\n",
      "   Log-prob: -24.0354 | Perplexity: 30.9890\n",
      "2. 'he likes bread and butter'\n",
      "   Log-prob: -25.2655 | Perplexity: 156.5062\n",
      "3. 'it's not all white and black'\n",
      "   Log-prob: -26.8648 | Perplexity: 46.4245\n",
      "4. 'the cat slept on the mat'\n",
      "   Log-prob: -28.4001 | Perplexity: 113.6767\n",
      "5. 'he likes butter and bread'\n",
      "   Log-prob: -29.0586 | Perplexity: 334.1950\n",
      "6. 'I turned on the radios and televisions'\n",
      "   Log-prob: -45.7345 | Perplexity: 161.0341\n",
      "7. 'I turned on the televisions and radios'\n",
      "   Log-prob: -52.0092 | Perplexity: 323.3701\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import math\n",
    "\n",
    "model_name = \"znhoughton/opt-babylm-125m-seed42\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_phrase_probability(text, model, tokenizer, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the probability of a phrase.\n",
    "    Returns: (total_log_prob, perplexity, token_probs)\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
    "        \n",
    "        # Get logits\n",
    "        logits = outputs.logits  # [1, seq_len, vocab_size]\n",
    "        \n",
    "        # Calculate probability for each token\n",
    "        token_log_probs = []\n",
    "        token_probs_list = []\n",
    "        \n",
    "        for pos in range(logits.shape[1] - 1):  # -1 because last token has no next token\n",
    "            # Logits at this position predict next token\n",
    "            next_token_logits = logits[0, pos, :]\n",
    "            next_token_id = encoded['input_ids'][0, pos + 1].item()\n",
    "            \n",
    "            # Get log probability of the actual next token\n",
    "            log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "            token_log_prob = log_probs[next_token_id].item()\n",
    "            token_prob = math.exp(token_log_prob)\n",
    "            \n",
    "            token_log_probs.append(token_log_prob)\n",
    "            token_probs_list.append({\n",
    "                'token': tokenizer.decode([next_token_id]),\n",
    "                'token_id': next_token_id,\n",
    "                'log_prob': token_log_prob,\n",
    "                'prob': token_prob\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Token: '{tokenizer.decode([next_token_id])}' | \"\n",
    "                      f\"Prob: {token_prob:.6f} | Log-prob: {token_log_prob:.4f}\")\n",
    "        \n",
    "        # Total log probability (sum of log probs)\n",
    "        total_log_prob = sum(token_log_probs)\n",
    "        \n",
    "        # Perplexity (from the model's loss)\n",
    "        perplexity = math.exp(outputs.loss.item())\n",
    "        \n",
    "        return total_log_prob, perplexity, token_probs_list\n",
    "\n",
    "# Compare different phrases\n",
    "phrases = [\n",
    "    \"he likes bread and butter\",\n",
    "    \"he likes butter and bread\",\n",
    "    \"the cat slept on the mat\",\n",
    "    \"it's not all black and white\",\n",
    "    \"it's not all white and black\",\n",
    "    \"I turned on the radios and televisions\",\n",
    "    \"I turned on the televisions and radios\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARING PHRASE PROBABILITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for phrase in phrases:\n",
    "    print(f\"\\nPhrase: '{phrase}'\")\n",
    "    log_prob, perplexity, token_probs = get_phrase_probability(phrase, model, tokenizer, verbose=True)\n",
    "    \n",
    "    # Convert log prob to actual probability\n",
    "    # Note: this will be a very small number!\n",
    "    probability = math.exp(log_prob)\n",
    "    \n",
    "    results.append({\n",
    "        'phrase': phrase,\n",
    "        'log_prob': log_prob,\n",
    "        'probability': probability,\n",
    "        'perplexity': perplexity,\n",
    "        'avg_token_prob': math.exp(log_prob / len(token_probs)) if token_probs else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"  → Total log-probability: {log_prob:.4f}\")\n",
    "    print(f\"  → Probability: {probability:.2e}\")\n",
    "    print(f\"  → Perplexity: {perplexity:.4f}\")\n",
    "    print(f\"  → Avg token probability: {results[-1]['avg_token_prob']:.6f}\")\n",
    "\n",
    "# Rank by probability\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANKING (Best to Worst)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results.sort(key=lambda x: x['log_prob'], reverse=True)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. '{result['phrase']}'\")\n",
    "    print(f\"   Log-prob: {result['log_prob']:.4f} | Perplexity: {result['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78ea568-14bc-40ec-8a8a-a45c809d8455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa71c9aa129846c990ec762aa697020c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38febcee476945e29657cdfbe2a42aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980c8fc12f3145a0b45b093922239593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d346a9d5c4acc9d1f9353940f16bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaa8e61cb85498b8c11accade41804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2821a4393c4a44a196bbd4fd0380abd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d1c30bc4145b9b4832fe3e3bcbc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271568a667bf4aa3822e7b870f5d3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARING PHRASE PROBABILITIES\n",
      "======================================================================\n",
      "\n",
      "Phrase: 'he likes bread and butter'\n",
      "  Token: 'he' | Prob: 0.003994 | Log-prob: -5.5230\n",
      "  Token: ' likes' | Prob: 0.001919 | Log-prob: -6.2558\n",
      "  Token: ' bread' | Prob: 0.000122 | Log-prob: -9.0086\n",
      "  Token: ' and' | Prob: 0.217836 | Log-prob: -1.5240\n",
      "  Token: ' butter' | Prob: 0.246719 | Log-prob: -1.3995\n",
      "  → Total log-probability: -23.7109\n",
      "  → Probability: 5.04e-11\n",
      "  → Perplexity: 114.6844\n",
      "  → Avg token probability: 0.008720\n",
      "\n",
      "Phrase: 'he likes butter and bread'\n",
      "  Token: 'he' | Prob: 0.003994 | Log-prob: -5.5230\n",
      "  Token: ' likes' | Prob: 0.001919 | Log-prob: -6.2558\n",
      "  Token: ' butter' | Prob: 0.000163 | Log-prob: -8.7243\n",
      "  Token: ' and' | Prob: 0.059628 | Log-prob: -2.8196\n",
      "  Token: ' bread' | Prob: 0.013843 | Log-prob: -4.2800\n",
      "  → Total log-probability: -27.6027\n",
      "  → Probability: 1.03e-12\n",
      "  → Perplexity: 249.7692\n",
      "  → Avg token probability: 0.004004\n",
      "\n",
      "Phrase: 'the cat slept on the mat'\n",
      "  Token: 'the' | Prob: 0.010527 | Log-prob: -4.5538\n",
      "  Token: ' cat' | Prob: 0.000731 | Log-prob: -7.2217\n",
      "  Token: ' slept' | Prob: 0.000018 | Log-prob: -10.9327\n",
      "  Token: ' on' | Prob: 0.124632 | Log-prob: -2.0824\n",
      "  Token: ' the' | Prob: 0.505220 | Log-prob: -0.6828\n",
      "  Token: ' mat' | Prob: 0.008334 | Log-prob: -4.7875\n",
      "  → Total log-probability: -30.2608\n",
      "  → Probability: 7.21e-14\n",
      "  → Perplexity: 155.0053\n",
      "  → Avg token probability: 0.006451\n",
      "\n",
      "Phrase: 'it's not all black and white'\n",
      "  Token: 'it' | Prob: 0.007921 | Log-prob: -4.8383\n",
      "  Token: ''s' | Prob: 0.717671 | Log-prob: -0.3317\n",
      "  Token: ' not' | Prob: 0.124152 | Log-prob: -2.0862\n",
      "  Token: ' all' | Prob: 0.010164 | Log-prob: -4.5889\n",
      "  Token: ' black' | Prob: 0.000476 | Log-prob: -7.6504\n",
      "  Token: ' and' | Prob: 0.112749 | Log-prob: -2.1826\n",
      "  Token: ' white' | Prob: 0.183195 | Log-prob: -1.6972\n",
      "  → Total log-probability: -23.3753\n",
      "  → Probability: 7.05e-11\n",
      "  → Perplexity: 28.2003\n",
      "  → Avg token probability: 0.035461\n",
      "\n",
      "Phrase: 'it's not all white and black'\n",
      "  Token: 'it' | Prob: 0.007921 | Log-prob: -4.8383\n",
      "  Token: ''s' | Prob: 0.717671 | Log-prob: -0.3317\n",
      "  Token: ' not' | Prob: 0.124152 | Log-prob: -2.0862\n",
      "  Token: ' all' | Prob: 0.010164 | Log-prob: -4.5889\n",
      "  Token: ' white' | Prob: 0.000844 | Log-prob: -7.0777\n",
      "  Token: ' and' | Prob: 0.077712 | Log-prob: -2.5548\n",
      "  Token: ' black' | Prob: 0.032719 | Log-prob: -3.4198\n",
      "  → Total log-probability: -24.8974\n",
      "  → Probability: 1.54e-11\n",
      "  → Perplexity: 35.0498\n",
      "  → Avg token probability: 0.028531\n",
      "\n",
      "Phrase: 'I turned on the radios and televisions'\n",
      "  Token: ' turned' | Prob: 0.000000 | Log-prob: -15.2058\n",
      "  Token: ' on' | Prob: 0.050513 | Log-prob: -2.9855\n",
      "  Token: ' the' | Prob: 0.147433 | Log-prob: -1.9144\n",
      "  Token: ' rad' | Prob: 0.000108 | Log-prob: -9.1343\n",
      "  Token: 'ios' | Prob: 0.000750 | Log-prob: -7.1955\n",
      "  Token: ' and' | Prob: 0.083602 | Log-prob: -2.4817\n",
      "  Token: ' tele' | Prob: 0.000334 | Log-prob: -8.0034\n",
      "  Token: 'v' | Prob: 0.069473 | Log-prob: -2.6668\n",
      "  Token: 'isions' | Prob: 0.155368 | Log-prob: -1.8620\n",
      "  → Total log-probability: -51.4494\n",
      "  → Probability: 4.53e-23\n",
      "  → Perplexity: 303.8687\n",
      "  → Avg token probability: 0.003291\n",
      "\n",
      "Phrase: 'I turned on the televisions and radios'\n",
      "  Token: ' turned' | Prob: 0.000000 | Log-prob: -15.2058\n",
      "  Token: ' on' | Prob: 0.050513 | Log-prob: -2.9855\n",
      "  Token: ' the' | Prob: 0.147433 | Log-prob: -1.9144\n",
      "  Token: ' tele' | Prob: 0.000094 | Log-prob: -9.2750\n",
      "  Token: 'v' | Prob: 0.008671 | Log-prob: -4.7478\n",
      "  Token: 'isions' | Prob: 0.017161 | Log-prob: -4.0651\n",
      "  Token: ' and' | Prob: 0.046576 | Log-prob: -3.0667\n",
      "  Token: ' rad' | Prob: 0.000284 | Log-prob: -8.1648\n",
      "  Token: 'ios' | Prob: 0.019778 | Log-prob: -3.9232\n",
      "  → Total log-probability: -53.3483\n",
      "  → Probability: 6.78e-24\n",
      "  → Perplexity: 375.2481\n",
      "  → Avg token probability: 0.002665\n",
      "\n",
      "======================================================================\n",
      "RANKING (Best to Worst)\n",
      "======================================================================\n",
      "1. 'it's not all black and white'\n",
      "   Log-prob: -23.3753 | Perplexity: 28.2003\n",
      "2. 'he likes bread and butter'\n",
      "   Log-prob: -23.7109 | Perplexity: 114.6844\n",
      "3. 'it's not all white and black'\n",
      "   Log-prob: -24.8974 | Perplexity: 35.0498\n",
      "4. 'he likes butter and bread'\n",
      "   Log-prob: -27.6027 | Perplexity: 249.7692\n",
      "5. 'the cat slept on the mat'\n",
      "   Log-prob: -30.2608 | Perplexity: 155.0053\n",
      "6. 'I turned on the radios and televisions'\n",
      "   Log-prob: -51.4494 | Perplexity: 303.8687\n",
      "7. 'I turned on the televisions and radios'\n",
      "   Log-prob: -53.3483 | Perplexity: 375.2481\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import math\n",
    "\n",
    "model_name = \"znhoughton/opt-babylm-1.3b-seed42\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def get_phrase_probability(text, model, tokenizer, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the probability of a phrase.\n",
    "    Returns: (total_log_prob, perplexity, token_probs)\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded, labels=encoded[\"input_ids\"])\n",
    "        \n",
    "        # Get logits\n",
    "        logits = outputs.logits  # [1, seq_len, vocab_size]\n",
    "        \n",
    "        # Calculate probability for each token\n",
    "        token_log_probs = []\n",
    "        token_probs_list = []\n",
    "        \n",
    "        for pos in range(logits.shape[1] - 1):  # -1 because last token has no next token\n",
    "            # Logits at this position predict next token\n",
    "            next_token_logits = logits[0, pos, :]\n",
    "            next_token_id = encoded['input_ids'][0, pos + 1].item()\n",
    "            \n",
    "            # Get log probability of the actual next token\n",
    "            log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
    "            token_log_prob = log_probs[next_token_id].item()\n",
    "            token_prob = math.exp(token_log_prob)\n",
    "            \n",
    "            token_log_probs.append(token_log_prob)\n",
    "            token_probs_list.append({\n",
    "                'token': tokenizer.decode([next_token_id]),\n",
    "                'token_id': next_token_id,\n",
    "                'log_prob': token_log_prob,\n",
    "                'prob': token_prob\n",
    "            })\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Token: '{tokenizer.decode([next_token_id])}' | \"\n",
    "                      f\"Prob: {token_prob:.6f} | Log-prob: {token_log_prob:.4f}\")\n",
    "        \n",
    "        # Total log probability (sum of log probs)\n",
    "        total_log_prob = sum(token_log_probs)\n",
    "        \n",
    "        # Perplexity (from the model's loss)\n",
    "        perplexity = math.exp(outputs.loss.item())\n",
    "        \n",
    "        return total_log_prob, perplexity, token_probs_list\n",
    "\n",
    "# Compare different phrases\n",
    "phrases = [\n",
    "    \"he likes bread and butter\",\n",
    "    \"he likes butter and bread\",\n",
    "    \"the cat slept on the mat\",\n",
    "    \"it's not all black and white\",\n",
    "    \"it's not all white and black\",\n",
    "    \"I turned on the radios and televisions\",\n",
    "    \"I turned on the televisions and radios\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARING PHRASE PROBABILITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for phrase in phrases:\n",
    "    print(f\"\\nPhrase: '{phrase}'\")\n",
    "    log_prob, perplexity, token_probs = get_phrase_probability(phrase, model, tokenizer, verbose=True)\n",
    "    \n",
    "    # Convert log prob to actual probability\n",
    "    # Note: this will be a very small number!\n",
    "    probability = math.exp(log_prob)\n",
    "    \n",
    "    results.append({\n",
    "        'phrase': phrase,\n",
    "        'log_prob': log_prob,\n",
    "        'probability': probability,\n",
    "        'perplexity': perplexity,\n",
    "        'avg_token_prob': math.exp(log_prob / len(token_probs)) if token_probs else 0\n",
    "    })\n",
    "    \n",
    "    print(f\"  → Total log-probability: {log_prob:.4f}\")\n",
    "    print(f\"  → Probability: {probability:.2e}\")\n",
    "    print(f\"  → Perplexity: {perplexity:.4f}\")\n",
    "    print(f\"  → Avg token probability: {results[-1]['avg_token_prob']:.6f}\")\n",
    "\n",
    "# Rank by probability\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANKING (Best to Worst)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results.sort(key=lambda x: x['log_prob'], reverse=True)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. '{result['phrase']}'\")\n",
    "    print(f\"   Log-prob: {result['log_prob']:.4f} | Perplexity: {result['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe7168-c47d-4fcd-ad72-505c80fc9134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
